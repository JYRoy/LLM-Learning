# FlashAttention1&2

- [FlashAttention1\&2](#flashattention12)
  - [Note](#note)
  - [Overview](#overview)
  - [FlashAttention1](#flashattention1)
  - [FlashAttention2](#flashattention2)
  - [References](#references)

## Note

I implement FlashAttention into GPT2. You could find it in [LLM-Learning/GPT2/model.py](../GPT2/model.py)

## Overview

FlashAttention1 named "Fast and Memory-Efficient Extract Attention with IO-Awareness". It proposes a "Fast", "Memory-Efficient" attention method, which has been included into PyTorch2.

FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning

## FlashAttention1

## FlashAttention2


## References

- [FlashAttention1](https://arxiv.org/pdf/2205.14135)
- [FlashAttention2](https://arxiv.org/pdf/2307.08691)
- [通透理解FlashAttention与FlashAttention2：全面降低显存读写、加快计算速度 by v_JULY_v](https://blog.csdn.net/v_JULY_v/article/details/133619540)